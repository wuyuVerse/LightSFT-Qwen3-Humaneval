# LLaMA Factory training configuration

defaults:
  - base

# LLaMA Factory specific settings
training:
  method: "llamafactory"
  model_name: "qwen3-8b"
  dataset_name: "custom_dataset"
  
  # LLaMA Factory specific parameters
  stage: "sft"
  finetuning_type: "lora"
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1
  lora_target: "all"
  
  # Training parameters
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2e-4
  num_train_epochs: 3
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Generation parameters
  max_length: 2048
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  do_sample: true
  
  # Save and evaluation
  save_strategy: "steps"
  save_steps: 500
  eval_strategy: "steps"
  eval_steps: 500
  logging_steps: 100
  save_total_limit: 3
  
  # Output
  output_dir: "outputs/models/llamafactory"
  logging_dir: "logs/llamafactory"
  
  # Data
  data_path: "outputs/data/sampled_data_llamafactory.jsonl"
  max_samples: null  # Use all samples
  
  # System prompt
  system_prompt: "You are a helpful coding assistant. Please provide accurate and helpful code solutions."
