#!/usr/bin/env python3
"""
æ¨¡å‹è¯„æµ‹è„šæœ¬ - ä½¿ç”¨EvalPlusè¿›è¡ŒHumanEvalè¯„æµ‹
æ”¯æŒç¦»çº¿æ¨¡å¼ï¼Œä½¿ç”¨æœ¬åœ°æ•°æ®é›†
"""

import subprocess
import sys
import yaml
import json
import os
from pathlib import Path
from datetime import datetime


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    project_root = Path(__file__).parent.parent.parent
    config_path = project_root / "config" / "eval.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def check_dataset():
    """æ£€æŸ¥HumanEval+æ•°æ®é›†æ˜¯å¦å­˜åœ¨"""
    print("=== æ£€æŸ¥HumanEval+æ•°æ®é›† ===")
    
    # ä½¿ç”¨é¡¹ç›®dataç›®å½•
    project_root = Path(__file__).parent.parent.parent
    data_dir = project_root / "data"
    dataset_path = data_dir / "HumanEvalPlus.jsonl"
    
    if dataset_path.exists():
        print(f"âœ“ æ•°æ®é›†å·²å­˜åœ¨: {dataset_path}")
        print(f"æ–‡ä»¶å¤§å°: {dataset_path.stat().st_size / 1024 / 1024:.2f} MB")
        return True
    else:
        print(f"âœ— æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        print("è¯·å…ˆåœ¨æœ‰ç½‘ç»œçš„ç¯å¢ƒä¸‹è¿è¡Œ: python scripts/download_dataset.py")
        return False


def run_evaluation(config):
    """è¿è¡Œè¯„æµ‹ - ä½¿ç”¨EvalPlus v0.3.1çš„ä¸€ä½“åŒ–å‘½ä»¤ (HumanEval+)"""
    eval_config = config['evaluation']
    
    # æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®šç»“æœå­˜å‚¨è·¯å¾„
    model_name = eval_config.get('model_name', 'unknown')
    eval_mode = eval_config.get('eval_mode', 'auto')  # base, chat, auto
    
    # è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼šæ ¹æ®æ¨¡å‹è·¯å¾„åˆ¤æ–­æ˜¯baseè¿˜æ˜¯chatæ¨¡å‹
    if eval_mode == 'auto':
        model_path = eval_config['model_path']
        if 'saves' in model_path or 'sft' in model_path or 'chat' in model_path:
            eval_mode = 'chat'
        else:
            eval_mode = 'base'
    
    # ä¸ºä¸åŒæ¨¡å¼è®¾ç½®ä¸åŒçš„ç»“æœç›®å½•ï¼Œé¿å…ç»“æœæ··æ·†
    base_results_root = eval_config.get('evalplus_root', 'evalplus_results')
    results_root = f"{base_results_root}_{eval_mode}"
    
    print(f"ğŸ“Š è¯„æµ‹æ¨¡å¼: {eval_mode} ({'å¾®è°ƒæ¨¡å‹' if eval_mode == 'chat' else 'åŸºç¡€æ¨¡å‹'})")
    print(f"ğŸ“ ç»“æœå­˜å‚¨è·¯å¾„: {results_root}")
    backend = eval_config['backend']
    # å¯é€‰é™åˆ¶å¯è§GPUï¼ˆå•å¡/å¤šå¡ï¼‰
    cuda_visible = eval_config.get('cuda_visible_devices')
    # å¯¹äº hf åç«¯ï¼Œé»˜è®¤å¼ºåˆ¶å•å¡ä»¥é¿å… accelerate å¤šå¡åˆ†ç‰‡å¯¼è‡´è®¾å¤‡ä¸ä¸€è‡´
    # å¯¹äº vllm åç«¯ï¼Œæ”¯æŒå¤šå¡å¼ é‡å¹¶è¡Œï¼Œæ€§èƒ½æ›´å¥½
    if cuda_visible is None and backend == 'hf':
        cuda_visible = '0'
    if cuda_visible is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = str(cuda_visible)
        print(f"è®¾ç½® CUDA_VISIBLE_DEVICES={cuda_visible} (åç«¯: {backend})")
    
    # æ„å»ºä¸€ä½“åŒ–è¯„æµ‹å‘½ä»¤ - ç›´æ¥ä½¿ç”¨evalplus.evaluateè¿›è¡Œä»£ç ç”Ÿæˆ+è¯„æµ‹
    # ç§»é™¤ --base_only ä»¥ä½¿ç”¨ HumanEval+ å…¨é‡æµ‹è¯•
    cmd = [
        "evalplus.evaluate",
        "--model", eval_config['model_path'],
        "--dataset", eval_config['benchmark'],
        "--backend", backend,
        "--greedy",
        "--root", str(results_root),
        "--force_base_prompt"
    ]
    # vLLM åç«¯æ”¯æŒå¼ é‡å¹¶è¡Œ --tpï¼Œå¤§å¹…æå‡æ¨ç†é€Ÿåº¦
    if backend == 'vllm':
        tp = eval_config.get('tp') or eval_config.get('tensor_parallel_size')
        if tp:
            cmd.extend(["--tp", str(tp)])
            print(f"ä½¿ç”¨ vLLM åç«¯ï¼Œå¼ é‡å¹¶è¡Œåº¦: {tp}")
        else:
            print("ä½¿ç”¨ vLLM åç«¯ï¼Œå•GPUæ¨¡å¼")
    elif backend == 'hf':
        print("ä½¿ç”¨ HuggingFace åç«¯ï¼ˆè¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨ vLLMï¼‰")
    
    print(f"è¿è¡Œä¸€ä½“åŒ–è¯„æµ‹å‘½ä»¤: {' '.join(cmd)}")
    if backend == 'vllm' and eval_config.get('tp', 1) > 1:
        print("ğŸš€ ä½¿ç”¨å¤šGPU vLLMåç«¯ï¼Œè¯„æµ‹é€Ÿåº¦å°†å¤§å¹…æå‡ï¼")
    elif backend == 'vllm':
        print("âš¡ ä½¿ç”¨å•GPU vLLMåç«¯ï¼Œæ¯”HFåç«¯æ›´å¿«")
    else:
        print("ğŸŒ ä½¿ç”¨HFåç«¯è¾ƒæ…¢ï¼Œå»ºè®®åˆ‡æ¢åˆ°vLLMåç«¯")
    print("æ³¨æ„: è¿™åŒ…æ‹¬ä»£ç ç”Ÿæˆå’Œè¯„æµ‹ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    try:
        # ä¸æ•è·è¾“å‡ºï¼Œè®©ç”¨æˆ·çœ‹åˆ°å®æ—¶è¿›åº¦
        result = subprocess.run(cmd, check=True)
        print("è¯„æµ‹å®Œæˆ!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"è¯„æµ‹å¤±è´¥: {e}")
        return False


def find_results(results_root: str):
    """æŸ¥æ‰¾è¯„æµ‹ç»“æœæ–‡ä»¶"""
    results_dir = Path(results_root)
    if not results_dir.exists():
        print(f"æœªæ‰¾åˆ°ç»“æœç›®å½•: {results_dir}")
        return None
    
    print(f"æŸ¥æ‰¾ç»“æœæ–‡ä»¶åœ¨: {results_dir}")
    
    # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
    result_files = list(results_dir.glob("**/*.json")) + list(results_dir.glob("**/*.jsonl"))
    if result_files:
        print(f"æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶:")
        for f in result_files:
            print(f"  - {f}")
        # è¿”å›æœ€æ–°çš„æ–‡ä»¶
        return max(result_files, key=lambda x: x.stat().st_mtime)
    return None


def generate_report(results_file, output_dir, eval_mode="unknown"):
    """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
    if not results_file or not results_file.exists():
        print("æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
        return
    
    try:
        print(f"è¯»å–ç»“æœæ–‡ä»¶: {results_file}")
        
        # è¯»å–ç»“æœ
        with open(results_file, 'r', encoding='utf-8') as f:
            if results_file.suffix == '.jsonl':
                results = [json.loads(line) for line in f]
            else:
                results = json.load(f)
        
        # ä¸ºä¸åŒæ¨¡å¼ç”Ÿæˆä¸åŒçš„æŠ¥å‘Šæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_filename = f"evaluation_report_{eval_mode}_{timestamp}.md"
        report_path = Path(output_dir) / report_filename
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# HumanEvalè¯„æµ‹æŠ¥å‘Š - {eval_mode.upper()}æ¨¡å‹\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**è¯„æµ‹æ¨¡å¼:** {eval_mode} ({'å¾®è°ƒæ¨¡å‹' if eval_mode == 'chat' else 'åŸºç¡€æ¨¡å‹'})\n")
            f.write(f"**ç»“æœæ–‡ä»¶:** {results_file}\n\n")
            
            # è®¡ç®—å’Œæ˜¾ç¤ºæŒ‡æ ‡
            eval_results = results
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…è£…æ ¼å¼ï¼ˆæœ‰date, hash, evalå­—æ®µï¼‰
            if isinstance(results, dict) and 'eval' in results:
                eval_results = results['eval']
            
            if isinstance(eval_results, dict) and any(task_id.startswith('HumanEval/') for task_id in eval_results.keys()):
                # å¤„ç†EvalPlusæ ¼å¼çš„ç»“æœ
                total_tasks = len(eval_results)
                base_pass = 0
                plus_pass = 0
                
                for task_id, task_results in eval_results.items():
                    if task_results and len(task_results) > 0:
                        result = task_results[0]  # å–ç¬¬ä¸€ä¸ªç»“æœ
                        if result.get('base_status') == 'pass':
                            base_pass += 1
                        if result.get('plus_status') == 'pass':
                            plus_pass += 1
                
                f.write("## ğŸ“Š è¯„æµ‹æŒ‡æ ‡è¯´æ˜\n\n")
                f.write("- **Pass@1 (base)**: åŸºç¡€HumanEvalæµ‹è¯•é›†çš„ä¸€æ¬¡é€šè¿‡ç‡\n")
                f.write("- **Pass@1 (plus)**: HumanEval+å¢å¼ºæµ‹è¯•é›†çš„ä¸€æ¬¡é€šè¿‡ç‡\n")
                f.write("- **åŸºç¡€æµ‹è¯•**: éªŒè¯ä»£ç çš„åŸºæœ¬åŠŸèƒ½æ­£ç¡®æ€§\n")
                f.write("- **å¢å¼ºæµ‹è¯•**: åŒ…å«æ›´å¤šè¾¹ç•Œæƒ…å†µå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œæ›´ä¸¥æ ¼\n\n")
                
                f.write("## ğŸ¯ è¯„æµ‹ç»“æœ\n\n")
                f.write(f"**æ€»é¢˜æ•°:** {total_tasks}\n")
                f.write(f"**Pass@1 (base):** {base_pass}/{total_tasks} = {base_pass/total_tasks:.1%}\n")
                f.write(f"**Pass@1 (plus):** {plus_pass}/{total_tasks} = {plus_pass/total_tasks:.1%}\n\n")
                
                # åˆ†ç±»ç»Ÿè®¡
                both_pass = sum(1 for task_results in eval_results.values() 
                              if task_results and len(task_results) > 0 and 
                              task_results[0].get('base_status') == 'pass' and 
                              task_results[0].get('plus_status') == 'pass')
                base_only = base_pass - both_pass
                plus_only = plus_pass - both_pass
                both_fail = total_tasks - base_pass - plus_only
                
                f.write("### ğŸ“ˆ é€šè¿‡æƒ…å†µåˆ†æ\n\n")
                f.write(f"- âœ… **ä¸¤ç§æµ‹è¯•éƒ½é€šè¿‡:** {both_pass} é¢˜ ({both_pass/total_tasks:.1%})\n")
                f.write(f"- ğŸŸ¡ **ä»…åŸºç¡€æµ‹è¯•é€šè¿‡:** {base_only} é¢˜ ({base_only/total_tasks:.1%})\n")
                f.write(f"- ğŸŸ  **ä»…å¢å¼ºæµ‹è¯•é€šè¿‡:** {plus_only} é¢˜ ({plus_only/total_tasks:.1%})\n")
                f.write(f"- âŒ **ä¸¤ç§æµ‹è¯•éƒ½å¤±è´¥:** {both_fail} é¢˜ ({both_fail/total_tasks:.1%})\n\n")
                
                # å¤±è´¥æ¡ˆä¾‹åˆ†æ
                failed_tasks = []
                for task_id, task_results in eval_results.items():
                    if task_results and len(task_results) > 0:
                        result = task_results[0]
                        if result.get('base_status') != 'pass' or result.get('plus_status') != 'pass':
                            failed_tasks.append((task_id, result))
                
                if failed_tasks:
                    f.write("### âŒ å¤±è´¥é¢˜ç›®åˆ†æ\n\n")
                    f.write("| é¢˜ç›®ID | åŸºç¡€æµ‹è¯• | å¢å¼ºæµ‹è¯• | å¤±è´¥åŸå›  |\n")
                    f.write("|--------|----------|----------|----------|\n")
                    for task_id, result in failed_tasks[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªå¤±è´¥æ¡ˆä¾‹
                        base_status = "âœ…" if result.get('base_status') == 'pass' else "âŒ"
                        plus_status = "âœ…" if result.get('plus_status') == 'pass' else "âŒ"
                        base_fails = result.get('base_fail_tests', [])
                        plus_fails = result.get('plus_fail_tests', [])
                        reason = ""
                        if base_fails:
                            reason = f"åŸºç¡€æµ‹è¯•å¤±è´¥: {len(base_fails)}ä¸ªç”¨ä¾‹"
                        if plus_fails:
                            if reason:
                                reason += f"; å¢å¼ºæµ‹è¯•å¤±è´¥: {len(plus_fails)}ä¸ªç”¨ä¾‹"
                            else:
                                reason = f"å¢å¼ºæµ‹è¯•å¤±è´¥: {len(plus_fails)}ä¸ªç”¨ä¾‹"
                        f.write(f"| {task_id} | {base_status} | {plus_status} | {reason} |\n")
                    
                    if len(failed_tasks) > 10:
                        f.write(f"\n*æ³¨ï¼šè¿˜æœ‰{len(failed_tasks)-10}ä¸ªå¤±è´¥é¢˜ç›®æœªåœ¨è¡¨æ ¼ä¸­æ˜¾ç¤º*\n")
                    f.write("\n")
            
            elif isinstance(results, list):
                # å¤„ç†åˆ—è¡¨æ ¼å¼çš„ç»“æœ
                total = len(results)
                passed = sum(1 for r in results if r.get('passed', False))
                f.write(f"**æ€»é¢˜æ•°:** {total}\n")
                f.write(f"**é€šè¿‡æ•°:** {passed}\n")
                f.write(f"**Pass@1:** {passed/total:.1%}\n\n")
            
            # æ·»åŠ æ¨¡å‹å¯¹æ¯”å»ºè®®
            f.write("## ğŸ’¡ æ€§èƒ½å»ºè®®\n\n")
            if eval_mode == 'base':
                f.write("- è¿™æ˜¯åŸºç¡€æ¨¡å‹çš„è¯„æµ‹ç»“æœï¼Œä½œä¸ºå¯¹æ¯”åŸºå‡†\n")
                f.write("- å»ºè®®ä¸å¾®è°ƒåçš„æ¨¡å‹ç»“æœè¿›è¡Œå¯¹æ¯”ï¼ŒæŸ¥çœ‹è®­ç»ƒæ•ˆæœ\n")
            elif eval_mode == 'chat':
                f.write("- è¿™æ˜¯å¾®è°ƒæ¨¡å‹çš„è¯„æµ‹ç»“æœ\n")
                f.write("- å¯ä»¥ä¸åŸºç¡€æ¨¡å‹å¯¹æ¯”ï¼Œè¯„ä¼°å¾®è°ƒçš„æå‡æ•ˆæœ\n")
                f.write("- å¦‚æœç»“æœä¸ç†æƒ³ï¼Œå»ºè®®æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡å’Œè®­ç»ƒå‚æ•°\n")
            f.write("\n")
            
            # ç®€åŒ–è¯¦ç»†ç»“æœæ˜¾ç¤ºï¼ˆåªæ˜¾ç¤ºæ‘˜è¦ï¼‰
            f.write("## ğŸ“‹ ç»“æœæ–‡ä»¶ä¿¡æ¯\n\n")
            f.write(f"å®Œæ•´ç»“æœæ•°æ®å·²ä¿å­˜åœ¨: `{results_file}`\n")
            f.write(f"ç»“æœæ ¼å¼: {'JSONL' if results_file.suffix == '.jsonl' else 'JSON'}\n")
            f.write(f"æ•°æ®å¤§å°: {len(str(results))} å­—ç¬¦\n")
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
    except Exception as e:
        print(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")


def setup_offline_env():
    """è®¾ç½®ç¦»çº¿ç¯å¢ƒå˜é‡"""
    project_root = Path(__file__).parent.parent.parent
    data_dir = project_root / "data"
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©EvalPlusä½¿ç”¨æœ¬åœ°æ•°æ®é›†
    os.environ['EVALPLUS_CACHE_DIR'] = str(data_dir)
    os.environ['HF_HOME'] = str(data_dir / "hf_cache")
    
    # è®¾ç½®HumanEval+è¦†ç›–è·¯å¾„ï¼Œä½¿ç”¨æˆ‘ä»¬ä¸‹è½½çš„HumanEval+æ–‡ä»¶
    humaneval_plus_path = data_dir / "HumanEvalPlus.jsonl"
    if humaneval_plus_path.exists():
        os.environ['HUMANEVAL_OVERRIDE_PATH'] = str(humaneval_plus_path)
        print(f"âœ“ è®¾ç½®ç¦»çº¿ç¯å¢ƒ")
        print(f"  EVALPLUS_CACHE_DIR: {data_dir}")
        print(f"  HF_HOME: {data_dir / 'hf_cache'}")
        print(f"  HUMANEVAL_OVERRIDE_PATH: {humaneval_plus_path}")
    else:
        print("âš  è­¦å‘Š: HumanEvalPlus.jsonlæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤ä¸‹è½½æ–¹å¼")
        print(f"âœ“ è®¾ç½®ç¦»çº¿ç¯å¢ƒ")
        print(f"  EVALPLUS_CACHE_DIR: {data_dir}")
        print(f"  HF_HOME: {data_dir / 'hf_cache'}")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    (data_dir / "hf_cache").mkdir(parents=True, exist_ok=True)


def main():
    """ä¸»å‡½æ•°"""
    print("=== HumanEvalæ¨¡å‹è¯„æµ‹ (ç¦»çº¿æ¨¡å¼) ===")
    print("ä½¿ç”¨EvalPlus v0.3.1è¿›è¡Œè¯„æµ‹")
    
    # è®¾ç½®ç¦»çº¿ç¯å¢ƒ
    setup_offline_env()
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
        print("âœ“ é…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âœ— é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = config['evaluation']['model_path']
    if not Path(model_path).exists():
        print(f"âœ— æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æ£€æŸ¥è·¯å¾„é…ç½®")
        return 1
    else:
        print(f"âœ“ æ¨¡å‹è·¯å¾„å­˜åœ¨: {model_path}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = config['evaluation']['output_dir']
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    print(f"âœ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # ç¡®å®šè¯„æµ‹æ¨¡å¼å’Œç»“æœå­˜å‚¨è·¯å¾„
    eval_config = config['evaluation']
    model_name = eval_config.get('model_name', 'unknown')
    eval_mode = eval_config.get('eval_mode', 'auto')  # base, chat, auto
    
    # è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼šæ ¹æ®æ¨¡å‹è·¯å¾„åˆ¤æ–­æ˜¯baseè¿˜æ˜¯chatæ¨¡å‹
    if eval_mode == 'auto':
        model_path = eval_config['model_path']
        if 'saves' in model_path or 'sft' in model_path or 'chat' in model_path:
            eval_mode = 'chat'
        else:
            eval_mode = 'base'
    
    # ä¸ºä¸åŒæ¨¡å¼è®¾ç½®ä¸åŒçš„ç»“æœç›®å½•ï¼Œé¿å…ç»“æœæ··æ·†
    base_results_root = eval_config.get('evalplus_root', 'evalplus_results')
    results_root = f"{base_results_root}_{eval_mode}"
    
    print(f"ğŸ“Š å½“å‰è¯„æµ‹æ¨¡å¼: {eval_mode} ({'å¾®è°ƒæ¨¡å‹' if eval_mode == 'chat' else 'åŸºç¡€æ¨¡å‹'})")
    print(f"ğŸ“ ç»“æœå­˜å‚¨è·¯å¾„: {results_root}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç”Ÿæˆçš„ç»“æœæ–‡ä»¶
    results_file = find_results(results_root)
    if results_file:
        print(f"âœ“ æ‰¾åˆ°å·²æœ‰çš„{eval_mode}æ¨¡å‹ç»“æœæ–‡ä»¶: {results_file}")
        print("ä½¿ç”¨å·²æœ‰ç»“æœæ–‡ä»¶ç”ŸæˆæŠ¥å‘Š...")
        generate_report(results_file, output_dir, eval_mode)
        return 0
    
    # æ£€æŸ¥æ•°æ®é›† (HumanEval+)
    print("\n=== æ£€æŸ¥æ•°æ®é›† ===")
    if not check_dataset():
        print("æ•°æ®é›†ä¸å­˜åœ¨ï¼Œæ— æ³•ç»§ç»­è¯„æµ‹")
        return 1
    
    # è¿è¡Œä¸€ä½“åŒ–è¯„æµ‹ï¼ˆä»£ç ç”Ÿæˆ+è¯„æµ‹ï¼‰
    print("\n=== å¼€å§‹ä¸€ä½“åŒ–è¯„æµ‹ ===")
    if not run_evaluation(config):
        print("âœ— è¯„æµ‹å¤±è´¥!")
        return 1
    
    print("âœ“ è¯„æµ‹æˆåŠŸå®Œæˆ!")
    
    # æŸ¥æ‰¾å¹¶å¤„ç†ç»“æœ
    results_file = find_results(results_root)
    if results_file:
        print(f"âœ“ æ‰¾åˆ°{eval_mode}æ¨¡å‹ç»“æœæ–‡ä»¶: {results_file}")
        generate_report(results_file, output_dir, eval_mode)
    else:
        print(f"âš  æœªæ‰¾åˆ°{eval_mode}æ¨¡å‹ç»“æœæ–‡ä»¶")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())