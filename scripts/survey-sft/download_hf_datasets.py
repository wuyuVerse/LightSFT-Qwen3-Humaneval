#!/usr/bin/env python3
"""
æ‰¹é‡ä¸‹è½½HuggingFaceä»£ç æ•°æ®é›†è„šæœ¬
è‡ªåŠ¨ä¸‹è½½å¤šä¸ªä»£ç ç›¸å…³æ•°æ®é›†åˆ°ç‹¬ç«‹æ–‡ä»¶å¤¹
"""

import os
import sys
import argparse
from pathlib import Path
from huggingface_hub import snapshot_download
from datetime import datetime


# å®šä¹‰æ‰€æœ‰éœ€è¦ä¸‹è½½çš„æ•°æ®é›†
DATASETS = [
    "codeparrot/apps",
    "mlfoundations-dev/stackexchange_codereview",
    "nampdn-ai/tiny-codes",
    "bigcode/commitpackft",
    "deepmind/code_contests",
    "SenseLLM/ReflectionSeq-GPT",
    "MatrixStudio/Codeforces-Python-Submissions",
    "Magpie-Align/Magpie-Qwen2.5-Coder-Pro-300K-v0.1",
    "bigcode/self-oss-instruct-sc2-exec-filter-50k",
    "PrimeIntellect/real-world-swe-problems",
    "lvwerra/stack-exchange-paired",
    "cfahlgren1/react-code-instructions",
    "PrimeIntellect/stackexchange-question-answering",
    "PrimeIntellect/SYNTHETIC-2-SFT-verified",
    "bugdaryan/sql-create-context-instruction",
]


def download_dataset(dataset_name: str, data_root: Path, resume: bool = True, 
                     skip_existing: bool = False, auto_yes: bool = False) -> bool:
    """
    ä¸‹è½½å•ä¸ªæ•°æ®é›†åˆ°æŒ‡å®šç›®å½•
    
    Args:
        dataset_name: HuggingFaceæ•°æ®é›†åç§° (æ ¼å¼: org/dataset)
        data_root: æ•°æ®æ ¹ç›®å½•
        resume: æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ 
        skip_existing: è‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®é›†
        auto_yes: éäº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨ç¡®è®¤æ‰€æœ‰æ“ä½œ
    
    Returns:
        ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    # æå–æ•°æ®é›†çŸ­åç§°ä½œä¸ºæ–‡ä»¶å¤¹åï¼ˆå»æ‰orgå‰ç¼€ï¼‰
    folder_name = dataset_name.split('/')[-1]
    target_dir = data_root / folder_name
    
    print(f"\n{'='*80}")
    print(f"ğŸ“¦ æ•°æ®é›†: {dataset_name}")
    print(f"ğŸ“ ç›®æ ‡ç›®å½•: {target_dir}")
    print(f"{'='*80}")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if target_dir.exists() and resume:
        print(f"âš ï¸  ç›®å½•å·²å­˜åœ¨: {target_dir}")
        if skip_existing or auto_yes:
            print("â­ï¸  è‡ªåŠ¨è·³è¿‡ï¼ˆç›®å½•å·²å­˜åœ¨ï¼‰")
            return True
        elif not sys.stdin.isatty():
            # åå°æ‰§è¡Œæ—¶é»˜è®¤è·³è¿‡å·²å­˜åœ¨çš„
            print("â­ï¸  åå°æ¨¡å¼ï¼šè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®é›†")
            return True
        else:
            response = input("æ˜¯å¦è·³è¿‡æ­¤æ•°æ®é›†ï¼Ÿ(y/nï¼Œé»˜è®¤n): ").strip().lower()
            if response == 'y':
                print("â­ï¸  è·³è¿‡")
                return True
    
    try:
        # ä½¿ç”¨snapshot_downloadä¸‹è½½æ•´ä¸ªæ•°æ®é›†
        start_time = datetime.now()
        print(f"â±ï¸  å¼€å§‹ä¸‹è½½: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        downloaded_path = snapshot_download(
            repo_id=dataset_name,
            repo_type="dataset",
            local_dir=str(target_dir),
            local_dir_use_symlinks=False,  # ç›´æ¥ä¸‹è½½æ–‡ä»¶ï¼Œä¸ä½¿ç”¨ç¬¦å·é“¾æ¥
            resume_download=resume,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            max_workers=4,  # å¹¶è¡Œä¸‹è½½çº¿ç¨‹æ•°
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"âœ… ä¸‹è½½æˆåŠŸ: {downloaded_path}")
        print(f"â±ï¸  è€—æ—¶: {duration:.2f}ç§’ ({duration/60:.2f}åˆ†é’Ÿ)")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤¹å¤§å°
        total_size = sum(f.stat().st_size for f in target_dir.rglob('*') if f.is_file())
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {total_size / (1024**3):.2f} GB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {dataset_name}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        return False


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡ä¸‹è½½HuggingFaceä»£ç æ•°æ®é›†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # äº¤äº’æ¨¡å¼ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
  python scripts/download_hf_datasets.py
  
  # åå°éäº¤äº’æ¨¡å¼ï¼ˆè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®é›†ï¼‰
  nohup python scripts/download_hf_datasets.py --auto-yes &
  
  # ä¸‹è½½æŒ‡å®šæ•°æ®é›†
  python scripts/download_hf_datasets.py --datasets codeparrot/apps bigcode/commitpackft
  
  # è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®é›†
  python scripts/download_hf_datasets.py --skip-existing
        """
    )
    parser.add_argument(
        "--auto-yes", "-y",
        action="store_true",
        help="éäº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨ç¡®è®¤æ‰€æœ‰æ“ä½œï¼ˆé€‚åˆåå°æ‰§è¡Œï¼‰"
    )
    parser.add_argument(
        "--skip-existing", "-s",
        action="store_true",
        help="è‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®é›†"
    )
    parser.add_argument(
        "--datasets", "-d",
        nargs="+",
        help="æŒ‡å®šè¦ä¸‹è½½çš„æ•°æ®é›†ï¼ˆé»˜è®¤ä¸‹è½½æ‰€æœ‰ï¼‰"
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        help="æ•°æ®ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤: <project_root>/dataï¼‰"
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="å¹¶è¡Œä¸‹è½½çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰"
    )
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("="*80)
    print("ğŸš€ HuggingFaceä»£ç æ•°æ®é›†æ‰¹é‡ä¸‹è½½å·¥å…·")
    print("="*80)
    
    # ç¡®å®šæ•°æ®æ ¹ç›®å½• (ä» scripts/survey-sft/ åˆ°é¡¹ç›®æ ¹ç›®å½•)
    project_root = Path(__file__).parent.parent.parent
    if args.data_dir:
        data_root = Path(args.data_dir)
    else:
        data_root = project_root / "data"
    data_root.mkdir(parents=True, exist_ok=True)
    
    # ç¡®å®šè¦ä¸‹è½½çš„æ•°æ®é›†
    if args.datasets:
        datasets_to_download = args.datasets
        print(f"\nğŸ“‹ æŒ‡å®šä¸‹è½½ {len(datasets_to_download)} ä¸ªæ•°æ®é›†")
    else:
        datasets_to_download = DATASETS
        print(f"\nğŸ“‹ å¾…ä¸‹è½½æ•°æ®é›†æ•°é‡: {len(datasets_to_download)}")
    
    print(f"ğŸ“‚ æ•°æ®æ ¹ç›®å½•: {data_root}")
    print(f"\næ•°æ®é›†åˆ—è¡¨:")
    for i, dataset in enumerate(datasets_to_download, 1):
        print(f"  {i:2d}. {dataset}")
    
    # ç¡®è®¤å¼€å§‹ä¸‹è½½
    print("\n" + "="*80)
    if args.auto_yes:
        print("âš¡ éäº¤äº’æ¨¡å¼ï¼šè‡ªåŠ¨å¼€å§‹ä¸‹è½½")
    elif not sys.stdin.isatty():
        print("âš¡ åå°æ¨¡å¼ï¼šè‡ªåŠ¨å¼€å§‹ä¸‹è½½")
    else:
        response = input("æ˜¯å¦å¼€å§‹ä¸‹è½½ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").strip().lower()
        if response == 'n':
            print("âŒ å–æ¶ˆä¸‹è½½")
            return 1
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = 0
    failed_datasets = []
    total_start_time = datetime.now()
    
    # é€ä¸ªä¸‹è½½æ•°æ®é›†
    for i, dataset_name in enumerate(datasets_to_download, 1):
        print(f"\n\n{'#'*80}")
        print(f"è¿›åº¦: [{i}/{len(datasets_to_download)}]")
        print(f"{'#'*80}")
        
        if download_dataset(
            dataset_name, 
            data_root, 
            skip_existing=args.skip_existing,
            auto_yes=args.auto_yes
        ):
            success_count += 1
        else:
            failed_datasets.append(dataset_name)
    
    # ä¸‹è½½æ€»ç»“
    total_duration = (datetime.now() - total_start_time).total_seconds()
    
    print("\n\n" + "="*80)
    print("ğŸ“Š ä¸‹è½½æ€»ç»“")
    print("="*80)
    print(f"âœ… æˆåŠŸ: {success_count}/{len(datasets_to_download)}")
    print(f"âŒ å¤±è´¥: {len(failed_datasets)}/{len(datasets_to_download)}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_duration/60:.2f}åˆ†é’Ÿ ({total_duration/3600:.2f}å°æ—¶)")
    
    if failed_datasets:
        print(f"\nâŒ å¤±è´¥çš„æ•°æ®é›†:")
        for dataset in failed_datasets:
            print(f"  - {dataset}")
        print(f"\nğŸ’¡ å¯ä»¥é‡æ–°è¿è¡Œæ­¤è„šæœ¬ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ")
    
    # è®¡ç®—æ€»æ•°æ®å¤§å°
    total_size = sum(f.stat().st_size for f in data_root.rglob('*') if f.is_file())
    print(f"\nğŸ“Š æ•°æ®æ€»å¤§å°: {total_size / (1024**3):.2f} GB")
    print(f"ğŸ“ æ•°æ®ä½ç½®: {data_root}")
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ!")
    print("="*80)
    
    return 0 if len(failed_datasets) == 0 else 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        print("ğŸ’¡ å¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­ä¸‹è½½ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
        sys.exit(130)

