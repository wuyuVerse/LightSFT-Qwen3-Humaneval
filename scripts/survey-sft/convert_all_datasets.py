#!/usr/bin/env python3
"""
ç²¾ç¡®è½¬æ¢15ä¸ªHuggingFaceæ•°æ®é›†ä¸ºLLaMA-Factoryæ ¼å¼
åŸºäºå®é™…æ•°æ®é›†æ ¼å¼è¿›è¡Œç²¾ç¡®è½¬æ¢
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
from tqdm import tqdm
import glob


def convert_apps(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ APPS æ•°æ®é›† - JSONLæ ¼å¼ï¼ŒåŒ…å«questionå’Œsolutions"""
    print(f"  è½¬æ¢ APPS æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(tqdm(f, desc=f"  å¤„ç† {file_path.name}")):
                if max_samples and len(converted) >= max_samples:
                    break
                try:
                    data = json.loads(line)
                    question = data.get("question", "")
                    solutions = json.loads(data.get("solutions", "[]"))
                    
                    if question and solutions and len(solutions) > 0:
                        converted.append({
                            "messages": [
                                {"role": "user", "content": f"Solve this programming problem:\n\n{question}"},
                                {"role": "assistant", "content": f"```python\n{solutions[0]}\n```"}
                            ]
                        })
                except Exception as e:
                    continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_tiny_codes(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ tiny-codes æ•°æ®é›† - Parquetæ ¼å¼"""
    print(f"  è½¬æ¢ tiny-codes æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                prompt = row.get('prompt', row.get('instruction', ''))
                response = row.get('response', row.get('output', row.get('code', '')))
                if prompt and response:
                    converted.append({
                        "messages": [
                            {"role": "user", "content": prompt},
                            {"role": "assistant", "content": response}
                        ]
                    })
            except:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_commitpackft(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ commitpackft æ•°æ®é›† - JSONLæ ¼å¼åœ¨å¤šä¸ªè¯­è¨€ç›®å½•ä¸‹"""
    print(f"  è½¬æ¢ commitpackft æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if max_samples and len(converted) >= max_samples:
                    break
                try:
                    data = json.loads(line)
                    commit_msg = data.get("subject", data.get("message", ""))
                    new_code = data.get("new_contents", "")
                    old_code = data.get("old_contents", "")
                    
                    if commit_msg and new_code:
                        if old_code:
                            prompt = f"Refactor the code based on: {commit_msg}\n\nOld code:\n```\n{old_code}\n```"
                        else:
                            prompt = f"Implement: {commit_msg}"
                        
                        converted.append({
                            "messages": [
                                {"role": "user", "content": prompt},
                                {"role": "assistant", "content": f"```\n{new_code}\n```"}
                            ]
                        })
                except:
                    continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_stackexchange_codereview(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ stackexchange_codereview - Parquetæ ¼å¼ï¼Œæœ‰conversationså­—æ®µï¼ˆnumpy.ndarrayç±»å‹ï¼‰"""
    print(f"  è½¬æ¢ stackexchange_codereview æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                # conversationså­—æ®µæ˜¯numpy.ndarrayç±»å‹
                if 'conversations' in row:
                    conversations = row['conversations']
                    # numpy.ndarrayè½¬list
                    if hasattr(conversations, 'tolist'):
                        conversations = conversations.tolist()
                    
                    if conversations and len(conversations) > 0:
                        messages = []
                        for msg in conversations:
                            role = "user" if msg.get('from') == 'human' else "assistant"
                            messages.append({"role": role, "content": msg.get('value', '')})
                        if messages and len(messages) >= 2:
                            converted.append({"messages": messages})
                # å¤‡ç”¨ï¼šä½¿ç”¨instructionå’Œcompletion
                elif 'instruction' in row and 'completion' in row and row['instruction'] and row['completion']:
                    converted.append({
                        "messages": [
                            {"role": "user", "content": str(row['instruction'])},
                            {"role": "assistant", "content": str(row['completion'])}
                        ]
                    })
            except Exception as e:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_code_contests(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ code_contests - Parquetæ ¼å¼ï¼Œsolutionsæ˜¯dictç±»å‹"""
    print(f"  è½¬æ¢ code_contests æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                description = row.get('description', '')
                solutions = row.get('solutions')
                
                if description and solutions:
                    # solutionsæ˜¯dictæ ¼å¼ï¼ŒåŒ…å«languageå’Œsolutionæ•°ç»„
                    if isinstance(solutions, dict):
                        solution_arr = solutions.get('solution', [])
                        # è½¬æ¢ä¸ºlistï¼ˆå¯èƒ½æ˜¯numpy.ndarrayï¼‰
                        if hasattr(solution_arr, 'tolist'):
                            solution_arr = solution_arr.tolist()
                        if solution_arr and len(solution_arr) > 0:
                            solution = solution_arr[0]
                            if solution:
                                converted.append({
                                    "messages": [
                                        {"role": "user", "content": f"Solve this competitive programming problem:\n\n{description}"},
                                        {"role": "assistant", "content": solution}
                                    ]
                                })
            except Exception as e:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_reflection_seq_gpt(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ ReflectionSeq-GPT - JSONLæ ¼å¼ï¼ŒåŒ…å«messageså­—æ®µ"""
    print(f"  è½¬æ¢ ReflectionSeq-GPT æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc=f"  å¤„ç† {file_path.name}"):
                if max_samples and len(converted) >= max_samples:
                    break
                try:
                    data = json.loads(line)
                    if 'messages' in data:
                        messages_raw = data['messages']
                        if isinstance(messages_raw, str):
                            messages_raw = json.loads(messages_raw)
                        
                        messages = []
                        for msg in messages_raw:
                            role = msg.get('role', 'user')
                            content = msg.get('content', '')
                            if isinstance(content, list):
                                # æå–textç±»å‹çš„content
                                text_content = ' '.join([c.get('content', '') for c in content if c.get('type') == 'text'])
                                content = text_content
                            if content:
                                messages.append({"role": role, "content": content})
                        
                        if messages:
                            converted.append({"messages": messages})
                except:
                    continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_codeforces(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ Codeforces-Python-Submissions - Parquetæ ¼å¼"""
    print(f"  è½¬æ¢ Codeforces-Python-Submissions æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                # ä¼˜å…ˆä½¿ç”¨promptå’Œresponseå­—æ®µï¼ˆå·²ç»æ ¼å¼åŒ–å¥½çš„ï¼‰
                if 'prompt' in row and 'response' in row and row['prompt'] and row['response']:
                    converted.append({
                        "messages": [
                            {"role": "user", "content": row['prompt']},
                            {"role": "assistant", "content": row['response']}
                        ]
                    })
                # å¤‡ç”¨ï¼šä½¿ç”¨problem-descriptionå’Œcode
                elif 'code' in row and row['code']:
                    problem = row.get('problem-description', row.get('title', ''))
                    code = row['code']
                    if problem:
                        prompt = f"Solve this Codeforces problem:\n\n{problem}"
                    else:
                        prompt = "Write a Python solution for this Codeforces problem."
                    
                    converted.append({
                        "messages": [
                            {"role": "user", "content": prompt},
                            {"role": "assistant", "content": f"```python\n{code}\n```"}
                        ]
                    })
            except:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_self_oss_instruct(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ self-oss-instruct-sc2-exec-filter-50k - Parquetæ ¼å¼ï¼Œinstruction/responseå­—æ®µ"""
    print(f"  è½¬æ¢ self-oss-instruct æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                instruction = row.get('instruction', row.get('prompt', ''))
                response = row.get('response', row.get('output', ''))
                
                if instruction and response:
                    converted.append({
                        "messages": [
                            {"role": "user", "content": instruction},
                            {"role": "assistant", "content": response}
                        ]
                    })
            except:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_swe_problems(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ real-world-swe-problems - Parquetæ ¼å¼"""
    print(f"  è½¬æ¢ real-world-swe-problems æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                prompt = row.get('prompt', '')
                solution = row.get('gold_standard_solution', '')
                
                if prompt and solution:
                    converted.append({
                        "messages": [
                            {"role": "user", "content": prompt},
                            {"role": "assistant", "content": solution}
                        ]
                    })
            except:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_stack_exchange_paired(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ stack-exchange-paired - Parquetæ ¼å¼"""
    print(f"  è½¬æ¢ stack-exchange-paired æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                question = row.get('question', '')
                response_j = row.get('response_j', '')
                response_k = row.get('response_k', '')
                
                response = response_j if response_j else response_k
                if question and response:
                    converted.append({
                        "messages": [
                            {"role": "user", "content": question},
                            {"role": "assistant", "content": response}
                        ]
                    })
            except:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_react_code_instructions(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ react-code-instructions - JSONLæ ¼å¼ï¼ŒåŒ…å«messageså­—æ®µ"""
    print(f"  è½¬æ¢ react-code-instructions æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc=f"  å¤„ç† {file_path.name}"):
                if max_samples and len(converted) >= max_samples:
                    break
                try:
                    data = json.loads(line)
                    if 'messages' in data:
                        messages = data['messages']
                        standardized = []
                        for msg in messages:
                            role = msg.get('role', 'user')
                            content = msg.get('content', '')
                            if role and content:
                                standardized.append({"role": role, "content": content})
                        if standardized:
                            converted.append({"messages": standardized})
                except:
                    continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_stackexchange_qa(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ stackexchange-question-answering - Parquetæ ¼å¼"""
    print(f"  è½¬æ¢ stackexchange-question-answering æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                prompt = row.get('prompt', '')
                answer = row.get('gold_standard_solution', '')
                
                if prompt and answer:
                    converted.append({
                        "messages": [
                            {"role": "user", "content": prompt},
                            {"role": "assistant", "content": answer}
                        ]
                    })
            except:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_synthetic_2_sft(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ SYNTHETIC-2-SFT-verified - Parquetæ ¼å¼ï¼ŒåŒ…å«messageså­—æ®µï¼ˆnumpy.ndarrayç±»å‹ï¼‰"""
    print(f"  è½¬æ¢ SYNTHETIC-2-SFT-verified æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                if 'messages' in row:
                    messages = row['messages']
                    # numpy.ndarrayè½¬list
                    if hasattr(messages, 'tolist'):
                        messages = messages.tolist()
                    
                    if messages and len(messages) > 0:
                        standardized = []
                        for msg in messages:
                            role = msg.get('role', 'user')
                            content = msg.get('content', '')
                            if role and content:
                                standardized.append({"role": role, "content": content})
                        if standardized and len(standardized) >= 2:
                            converted.append({"messages": standardized})
            except Exception as e:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_sql_context(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ sql-create-context-instruction - Parquetæ ¼å¼ï¼Œtextå­—æ®µä½¿ç”¨[INST]...[/INST]æ ¼å¼"""
    print(f"  è½¬æ¢ sql-create-context-instruction æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                text = row.get('text', '')
                # textæ ¼å¼: [INST] instruction [/INST] response
                if text and '[INST]' in text and '[/INST]' in text:
                    # è§£æ [INST]...[/INST] æ ¼å¼
                    inst_start = text.find('[INST]')
                    inst_end = text.find('[/INST]')
                    if inst_start != -1 and inst_end != -1 and inst_end > inst_start:
                        instruction = text[inst_start + 6:inst_end].strip()
                        response = text[inst_end + 7:].strip()
                        
                        if instruction and response:
                            converted.append({
                                "messages": [
                                    {"role": "user", "content": instruction},
                                    {"role": "assistant", "content": response}
                                ]
                            })
            except Exception as e:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


def convert_magpie_qwen(input_files: List[Path], output_file: Path, max_samples: Optional[int] = None):
    """è½¬æ¢ Magpie-Qwen2.5-Coder-Pro-300K - Parquetæ ¼å¼ï¼Œconversationså­—æ®µï¼ˆnumpy.ndarrayç±»å‹ï¼‰"""
    print(f"  è½¬æ¢ Magpie-Qwen2.5-Coder-Pro-300K æ•°æ®é›†...")
    converted = []
    
    for file_path in input_files:
        if max_samples and len(converted) >= max_samples:
            break
        df = pd.read_parquet(file_path)
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"  å¤„ç† {file_path.name}"):
            if max_samples and len(converted) >= max_samples:
                break
            try:
                # conversationså­—æ®µæ˜¯numpy.ndarrayç±»å‹
                if 'conversations' in row:
                    conversations = row['conversations']
                    # numpy.ndarrayè½¬list
                    if hasattr(conversations, 'tolist'):
                        conversations = conversations.tolist()
                    
                    if conversations and len(conversations) > 0:
                        messages = []
                        for msg in conversations:
                            role_map = {'human': 'user', 'gpt': 'assistant', 'user': 'user', 'assistant': 'assistant'}
                            role = role_map.get(msg.get('from', 'user'), 'user')
                            content = msg.get('value', '')
                            if content:
                                messages.append({"role": role, "content": content})
                        if messages and len(messages) >= 2:
                            converted.append({"messages": messages})
                # å¤‡ç”¨ï¼šä½¿ç”¨instructionå’Œresponse
                elif 'instruction' in row and 'response' in row and row['instruction'] and row['response']:
                    converted.append({
                        "messages": [
                            {"role": "user", "content": str(row['instruction'])},
                            {"role": "assistant", "content": str(row['response'])}
                        ]
                    })
            except Exception as e:
                continue
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in converted:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(converted)


# æ•°æ®é›†é…ç½®
DATASETS_CONFIG = {
    "apps": {
        "converter": convert_apps,
        "pattern": "*.jsonl"
    },
    "tiny-codes": {
        "converter": convert_tiny_codes,
        "pattern": "*.parquet"
    },
    "commitpackft": {
        "converter": convert_commitpackft,
        "pattern": "data/**/data.jsonl"
    },
    "stackexchange_codereview": {
        "converter": convert_stackexchange_codereview,
        "pattern": "data/*.parquet"
    },
    "code_contests": {
        "converter": convert_code_contests,
        "pattern": "data/*.parquet"
    },
    "ReflectionSeq-GPT": {
        "converter": convert_reflection_seq_gpt,
        "pattern": "*.jsonl"
    },
    "Codeforces-Python-Submissions": {
        "converter": convert_codeforces,
        "pattern": "data/*.parquet"
    },
    "self-oss-instruct-sc2-exec-filter-50k": {
        "converter": convert_self_oss_instruct,
        "pattern": "data/*.parquet"
    },
    "real-world-swe-problems": {
        "converter": convert_swe_problems,
        "pattern": "data/*.parquet"
    },
    "stack-exchange-paired": {
        "converter": convert_stack_exchange_paired,
        "pattern": "data/**/*.parquet"
    },
    "react-code-instructions": {
        "converter": convert_react_code_instructions,
        "pattern": "data/*.jsonl"
    },
    "stackexchange-question-answering": {
        "converter": convert_stackexchange_qa,
        "pattern": "data/*.parquet"
    },
    "SYNTHETIC-2-SFT-verified": {
        "converter": convert_synthetic_2_sft,
        "pattern": "data/*.parquet"
    },
    "sql-create-context-instruction": {
        "converter": convert_sql_context,
        "pattern": "data/*.parquet"
    },
    "Magpie-Qwen2.5-Coder-Pro-300K-v0.1": {
        "converter": convert_magpie_qwen,
        "pattern": "data/*.parquet"
    }
}


def main():
    parser = argparse.ArgumentParser(description="è½¬æ¢15ä¸ªä»£ç æ•°æ®é›†ä¸ºLLaMA-Factoryæ ¼å¼")
    
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œä» scripts/survey-sft/ åˆ°é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent.parent
    default_data_dir = project_root / "data"
    default_output_dir = project_root / "data" / "llamafactory"
    
    parser.add_argument("--data-dir", type=str, default=str(default_data_dir), help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--output-dir", type=str, default=str(default_output_dir), help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max-samples", type=int, default=None, help="æ¯ä¸ªæ•°æ®é›†æœ€å¤šè½¬æ¢çš„æ ·æœ¬æ•°")
    parser.add_argument("--datasets", nargs="+", help="æŒ‡å®šè¦è½¬æ¢çš„æ•°æ®é›†")
    
    args = parser.parse_args()
    
    data_root = Path(args.data_dir)
    output_root = Path(args.output_dir)
    output_root.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("ğŸ”„ è½¬æ¢15ä¸ªä»£ç æ•°æ®é›†ä¸º LLaMA-Factory æ ¼å¼")
    print("="*80)
    print(f"ğŸ“‚ æ•°æ®æ ¹ç›®å½•: {data_root}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_root}")
    if args.max_samples:
        print(f"ğŸ“Š æ¯ä¸ªæ•°æ®é›†æœ€å¤š: {args.max_samples} æ¡")
    print()
    
    # ç¡®å®šè¦è½¬æ¢çš„æ•°æ®é›†
    if args.datasets:
        datasets = {k: v for k, v in DATASETS_CONFIG.items() if k in args.datasets}
    else:
        datasets = DATASETS_CONFIG
    
    results = {}
    dataset_info = {}
    
    for dataset_name, config in datasets.items():
        print(f"\n{'='*80}")
        print(f"ğŸ“¦ å¤„ç†æ•°æ®é›†: {dataset_name}")
        print(f"{'='*80}")
        
        dataset_dir = data_root / dataset_name
        if not dataset_dir.exists():
            print(f"  âš ï¸  ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")
            continue
        
        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        pattern = config["pattern"]
        input_files = list(dataset_dir.glob(pattern))
        
        if not input_files:
            print(f"  âš ï¸  æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {pattern}")
            continue
        
        print(f"  ğŸ“ æ‰¾åˆ° {len(input_files)} ä¸ªæ–‡ä»¶")
        
        # è½¬æ¢æ•°æ®é›†
        output_file = output_root / f"{dataset_name}.jsonl"
        try:
            count = config["converter"](input_files, output_file, args.max_samples)
            results[dataset_name] = count
            
            if count > 0:
                print(f"  âœ… è½¬æ¢æˆåŠŸ: {count} æ¡æ•°æ®")
                print(f"  ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
                print(f"  ğŸ“Š æ–‡ä»¶å¤§å°: {output_file.stat().st_size / (1024**2):.2f} MB")
                
                # æ·»åŠ åˆ°dataset_info
                dataset_key = dataset_name.replace('-', '_').replace('.', '_').lower()
                dataset_info[dataset_key] = {
                    "file_name": f"llamafactory/{dataset_name}.jsonl",
                    "formatting": "sharegpt",
                    "columns": {
                        "messages": "messages"
                    },
                    "tags": {
                        "role_tag": "role",
                        "content_tag": "content",
                        "user_tag": "user",
                        "assistant_tag": "assistant"
                    }
                }
            else:
                print(f"  âš ï¸  è½¬æ¢å¤±è´¥: 0 æ¡æ•°æ®")
        except Exception as e:
            print(f"  âŒ è½¬æ¢å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # æ›´æ–°dataset_info.json
    dataset_info_path = data_root / "dataset_info.json"
    
    # è¯»å–ç°æœ‰é…ç½®
    if dataset_info_path.exists():
        with open(dataset_info_path, 'r', encoding='utf-8') as f:
            existing_info = json.load(f)
    else:
        existing_info = {}
    
    # åˆå¹¶é…ç½®
    existing_info.update(dataset_info)
    
    # ä¿å­˜é…ç½®
    with open(dataset_info_path, 'w', encoding='utf-8') as f:
        json.dump(existing_info, f, ensure_ascii=False, indent=2)
    
    print(f"\n\n{'='*80}")
    print("ğŸ“Š è½¬æ¢æ€»ç»“")
    print(f"{'='*80}")
    
    total_samples = sum(results.values())
    print(f"âœ… æˆåŠŸè½¬æ¢ {len(results)} ä¸ªæ•°æ®é›†ï¼Œå…± {total_samples:,} æ¡æ•°æ®")
    print(f"\næ•°æ®é›†æ˜ç»†:")
    for dataset_name, count in results.items():
        print(f"  - {dataset_name}: {count:,} æ¡")
    
    print(f"\nğŸ“„ dataset_info.json å·²æ›´æ–°: {dataset_info_path}")
    print(f"ğŸ“ æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {output_root}")
    print(f"\nğŸ’¡ åœ¨ LLaMA-Factory ä¸­ä½¿ç”¨è¿™äº›æ•°æ®é›†:")
    print(f"   dataset: " + ",".join(list(dataset_info.keys())[:3]) + ",...")
    print("="*80)


if __name__ == "__main__":
    import sys
    sys.exit(main())

